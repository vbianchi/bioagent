# Configuration settings for the BioAgent

# LLM Provider ('openai', 'gemini', 'ollama')
llm_provider: "gemini" # CHANGE THIS to 'gemini' or 'ollama' as needed

# LLM Settings (Provider-specific model names)
llm_settings:
  # --- OpenAI ---
  openai_model_name: "gpt-3.5-turbo"
  # --- Google Gemini ---
  # Example: "gemini-pro", "gemini-1.5-pro-latest"
  gemini_model_name: "gemini-2.0-flash-lite"
  # --- Ollama ---
  # Specify the model name you have pulled in Ollama (e.g., "llama3", "mistral")
  ollama_model_name: "gemma3"
  # Optional: Specify Ollama base URL if not default (http://localhost:11434)
  # ollama_base_url: "http://localhost:11434"

  # --- Common Settings ---
  # Temperature for LLM calls (0 = deterministic, >0 = creative)
  temperature: 0

# Search Settings
search_settings:
  max_results_per_source: 3
  max_abstracts_to_summarize: 3

# Prompt Templates (Use | for multi-line strings preserving newlines)
prompts:
  routing_prompt: |
    Classify the user's query into one of the following categories: 'literature_search' or 'chat'. Respond ONLY with the category name.

    - 'literature_search': User is asking to find papers, articles, publications, search results, or specific information likely found by searching scientific literature databases (like PubMed or ArXiv). Keywords: "find papers", "search for articles", "publications on", "literature about".
    - 'chat': User is asking a general question, definition, explanation, making a statement, greeting, or having a conversation. Keywords: "what is", "explain", "tell me about", "hello", "hi", "can you". Also includes follow-up questions like "what did you find?".

    User Query: "{query}"
    Classification:
  refinement_prompt: |
    Given the user's query, extract the core topic or keywords suitable for searching scientific databases like PubMed and ArXiv. Focus on nouns, technical terms, essential concepts. Remove conversational phrases like "find papers on", "search for", "tell me about". Respond ONLY with the refined search query string.

    User Query: "{query}"
    Refined Search Query:
  summarization_prompt: |
    Given the user's original query and the following abstracts from search results, provide a concise summary of the key findings relevant to the query.

    Original User Query: "{query}"

    Abstracts:
    ---
    {abstracts_text}
    ---

    Concise Summary:
  # Basic chat prompt (less critical now history is used directly)
  chat_prompt: |
    User query: {query}
    Assistant response: